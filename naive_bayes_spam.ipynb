{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm and Spam Detection\n",
    "#### by Palermo Penano\n",
    "This notebook implements and describes the Naive Bayes algorithm for spam detection. It works through the entire pipeline of retrieving the data, preprocessing, applying the Bag of Words model to create the features, training the model using the scikit-learn package, and evaluating the model using the appropriate metric for unbalanced classes.\n",
    "\n",
    "I used the tutorial in this link as a guide: https://github.com/udacity/machine-learning/blob/master/projects/practice_projects/naive_bayes_tutorial/Naive_Bayes_tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "# for importing zipfiles from url\n",
    "from requests import get\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# scikit-learn modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and import into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download zipfile from website\n",
    "data_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "request = get(data_path)\n",
    "data_downloaded = BytesIO(request.content)\n",
    "zip_file = ZipFile(data_downloaded)\n",
    "\n",
    "# reading the first file in the zip file and decode raw bytes in the string \n",
    "spam_data = zip_file.read('SMSSpamCollection').decode('utf-8')\n",
    "\n",
    "# write data to a file\n",
    "with open('spam_data.txt', 'w') as f:\n",
    "    f.write(spam_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import into a pandas dataframe\n",
    "df = pd.read_csv('spam_data.txt', \n",
    "                 sep='\\t',\n",
    "                 header=None,\n",
    "                 names=['label', 'sms_message']\n",
    "                )\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string labels into binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "Machine learning algorithms needs numeric features for training. To get numeric features from a collection of strings, use the Bag of Words model to convert the sentences into a series of columns containing the frequency of each word for each sentence in the data.\n",
    "\n",
    "The steps to convert text into features using the Bag of Words model:\n",
    "1. Convert all string into lower case form\n",
    "2. Remove all punctuations\n",
    "3. Tokenize sentences (split up sentences into individual words using a delimiter and save to list)\n",
    "4. Count occurence of each word for each sentence using data generated in 3.\n",
    "\n",
    "Example below demonstrates how to do this using the CountVectorizer class in sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 1 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 2 0 0 0 0 0 1 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']\n",
    "\n",
    "count_vector = CountVectorizer(lowercase=True)\n",
    "\n",
    "# learn vocabulary\n",
    "count_vector.fit(documents)\n",
    "\n",
    "# transform documents to a document-term matrix, then to an array\n",
    "doc_array = count_vector.transform(documents).toarray()\n",
    "print(doc_array)\n",
    "\n",
    "# load into dataframe\n",
    "feature_names = count_vector.get_feature_names()\n",
    "frequency_matrix = pd.DataFrame(doc_array, \n",
    "                                columns=feature_names)\n",
    "frequency_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing\n",
    "Split training and label data into a training set (75%) and a test set (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data size: 5572\n",
      "Training set size: 4179\n",
      "Test set size: 1393\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'],\n",
    "                                                    df['label'],\n",
    "                                                    random_state=1\n",
    "                                                   )\n",
    "\n",
    "print(\"Full data size:\", df.shape[0])\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Bag of Words processing to the split data\n",
    "We fit the vocabulary dictionary only for the training data. Here's a great explanation why we do this: https://sebastianraschka.com/faq/docs/scale-training-test.html. The basic idea is that we treat test data as new and unseen. Therefore, we cannot learn--by applying the fit() function--the vocabulary of this new data and must use the vocabulary learned from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer()\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that training_data and testing_data are sparse matrices containing the frequency distribution of each word from the learned vocabulary using the training data. To get the pandas dataframe, we must pass this into the DataFrame method in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in learned dictionary from training_data: 7456\n"
     ]
    }
   ],
   "source": [
    "spam_feature_names = count_vector.get_feature_names()\n",
    "spam_frequency_matrix = pd.DataFrame(training_data.toarray(), \n",
    "                                columns=feature_names)\n",
    "\n",
    "print(\"Number of words in learned dictionary from training_data:\", len(spam_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem review\n",
    "\n",
    "The example in the tutorial is on finding the odds a person will have diabetes given that they tested positive for it. Let the probability of having diabetes, denoted by $P(D)$, be 0.01. This is often referred to as the __base rate__ or the __prior__ probability. Assume that the test is %90 accurate. This means that the probability of testing positive given that you have diabetes is $P(Pos|D) = 0.9$. Alternatively, the probabily of testing negative given the person doesn't have diabetes is $P(Neg|~D) = 0.9$.\n",
    "\n",
    "The probability we are interested in is\n",
    "\n",
    "$$P(D|Pos) = \\frac{P(Pos|D)P(D)}{P(Pos)}$$\n",
    "\n",
    "$P(Pos)$ is the probability of testing positive regardless of whether you have diabetes or not. People without diabetes can still test positive because the test is imperfect. The formula for $P(Pos)$ is\n",
    "\n",
    "$$P(Pos) = P(D)P(Pos|D) + P({\\sim}D)P(Pos|{\\sim}D)$$\n",
    "\n",
    "Intuitively, $P(D)P(Pos|D)$ is the probability of testing positive _and_ having diabetes. $P(~D)P(Pos|~D)$ is the probability of still testing positive _and_ not having diabetes. Their sum, therefore, is just the probability of testing positive, whether or not you have diabetes.\n",
    "\n",
    "Putting all of this together the probability of having diabetes given that you tested positive is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_d = 0.01\n",
    "prob_pos_d = 0.9\n",
    "prob_neg_no_d = 0.9\n",
    "prob_pos = prob_d * prob_pos_d + (1 - prob_d) * (1 - prob_neg_no_d)\n",
    "\n",
    "prob_d_pos = (prob_pos_d * prob_d) / prob_pos\n",
    "\n",
    "print(prob_d_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Despite the test being accurate 90% of the time, your probability of actually having diabetes is quite low. This is because the base rate incidence of having diabetes, independent of whether you tested positive or not, is only 1%. \n",
    "\n",
    "In this example, there is only feature with which we can use to predict whether you have diabetes. It is conceivable that many other features can predict the disease. To extend our example, suppose we also had data on whether your parents also had diabetes. The posterior probability is now as follows\n",
    "\n",
    "$$P(D|Pos, FamHist) = \\frac{P(Pos, FamHist|D)P(D)}{P(Pos, FamHist)}$$\n",
    "\n",
    "where $FamHist$ is an indicator equal to 1 if either one of your mother or father had diabetes. $P(Pos, FamHist)$ is the joint probability of a given individual having some outcome of a a test result and their family history. Repeatedly applying conditional probabilities, we can rewrite the object in the numerator as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(Pos, FamHist|D)P(D) & = P(Pos, FamHist, D)  \\\\\n",
    " & = P(Pos|FamHist, D)P(FamHist, D)  \\\\\n",
    " & = P(Pos|FamHist, D)P(FamHist|D)P(D)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This can get unwieldy if we had 3, or 4, or in most cases, 10 or more features. As a simplification, assume that features are _conditionally_ independent given that you have diabetes. In other words, $P(Pos|FamHist, D) = P(Pos|D)$. The formula then simplifies to\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(Pos, FamHist|D)P(D) & = P(Pos|D)P(FamHist|D)P(D)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This is much easier to work with because with N features the formula is just\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(x_1,...,x_N|D)P(D) & = \\prod_{i=1}^{N}P(x_i|D)P(D)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "which is an formula to work with than if we didn't make the conditional independence assumption. \n",
    "\n",
    "This assumption is what makes this formulation of the Bayes Theorem naive. It is naive because it is conceivable that ones family history of diabetes, for whatever reason, affects the outcome of the test results. Perhaps this is not the best example for this, but imagine we are trying to predict whether house price will be high or low (based on some exogenously established threshold) given the number of rooms and the size of the house in square feet. The size of your house certainly affects how many rooms your house will have, thus, it unreasonable to assume that number of rooms is conditionally independent of house size. Making this assumption is the price we pay to make our formula tractable.\n",
    "\n",
    "Going back to the diabetes example, the final formula of the posterior probability given conditionally independent features is\n",
    "\n",
    "$$P(D|Pos, FamHist) = \\frac{P(Pos|D)P(FamHist|D)P(D)}{P(Pos, FamHist)}$$\n",
    "\n",
    "The denominator is just $P(Pos, FamHist) = P(D)P(Pos,FamHist|D) + P({\\sim}D)P(Pos,FamHist|{\\sim}D)$\n",
    "\n",
    "In summary, Naive Bayes is just an extension of Bayes Theorem where the features are conditionally indepedent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes implementation using scikit-learn\n",
    "\n",
    "Let's go back to our original problem of spam classification and train a multinomial naive bayes model using the training data. In our application, the features are the frequency of each word learned by when we applied the Bag of Words model to create our training data. There are 7456 words, which means we have 7456 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train naive bayes classifier using training data\n",
    "# training data is a sparse matrix\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions using test data\n",
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "There are four metrics we can use to evaluate the effectiveness of our model. The metric we use depends on how balanced our classifications are. The metrics are\n",
    "\n",
    "1. Accuracy: Number of correct predictions / total number of predictions\n",
    "2. Precision: Number of correct predictions / total number of examples predicted to be spam\n",
    "3. Recall: Number of correct predicitons / total number of examples that are actually spam\n",
    "4. F1 Score: harmonic mean of Precision and Recall\n",
    "\n",
    "With highly unbalanced classification, accuracy is a poor metric. If out of 100 examples 3 are spam, we can predict every example to be not spam, in which case, we achieve an accuracy rating of 97%. Recall, however, would be zero (and precision, infinite). Because of the trade off between precision and recall, a better metric would be to use the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4179.000000\n",
       "mean        0.134482\n",
       "std         0.341210\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3617\n",
       "1     562\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of our classification, only about 13.5% of our messages are labeled spam. This suggests that we should consider using the F1 Score as our metric. We calculate them all any way for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9885139985642498\n",
      "Precision score:  0.9720670391061452\n",
      "Recall score:  0.9405405405405406\n",
      "F1 score:  0.9560439560439562\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
