{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm and Spam Detection\n",
    "\n",
    "This notebook is my implementation of the Naive Bayes algorithm for spam detection. I used the tutorial in this link as a guide: https://github.com/udacity/machine-learning/blob/master/projects/practice_projects/naive_bayes_tutorial/Naive_Bayes_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "# for importing zipfiles from url\n",
    "from requests import get\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data and import into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download zipfile from website\n",
    "data_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "request = get(data_path)\n",
    "data_downloaded = BytesIO(request.content)\n",
    "zip_file = ZipFile(data_downloaded)\n",
    "\n",
    "# reading the first file in the zip file and decode raw bytes in the string \n",
    "spam_data = zip_file.read('SMSSpamCollection').decode('utf-8')\n",
    "\n",
    "# write data to a file\n",
    "with open('spam_data.txt', 'w') as f:\n",
    "    f.write(spam_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import into a pandas dataframe\n",
    "df = pd.read_csv('spam_data.txt', \n",
    "                 sep='\\t',\n",
    "                 header=None,\n",
    "                 names=['label', 'sms_message']\n",
    "                )\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string labels into binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "Machine learning algorithms needs numeric features for training. To get numeric features, use the Bag of Words model to convert the sentences into a series of columns containing the frequency of each word for each sentence in the data.\n",
    "\n",
    "The steps to convert text into features using the Bag of Words model:\n",
    "1. Convert all string into lower case form\n",
    "2. Remove all punctuations\n",
    "3. Tokenize sentences (split up sentences into individual words using a delimiter and save to list)\n",
    "4. Count occurence of each word for each sentence using data generated in 3.\n",
    "\n",
    "Example below demonstrates how to do this using the CountVectorizer class in sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 1 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 2 0 0 0 0 0 1 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']\n",
    "\n",
    "count_vector = CountVectorizer(lowercase=True)\n",
    "\n",
    "# learn vocabulary\n",
    "count_vector.fit(documents)\n",
    "\n",
    "# transform documents to a document-term matrix, then to an array\n",
    "doc_array = count_vector.transform(documents).toarray()\n",
    "print(doc_array)\n",
    "\n",
    "# load into dataframe\n",
    "feature_names = count_vector.get_feature_names()\n",
    "frequency_matrix = pd.DataFrame(doc_array, \n",
    "                                columns=feature_names)\n",
    "frequency_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing\n",
    "Split training and label data into a training set (75%) and a test set (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data size: 5572\n",
      "Training set size: 4179\n",
      "Test set size: 1393\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'],\n",
    "                                                    df['label'],\n",
    "                                                    random_state=1\n",
    "                                                   )\n",
    "\n",
    "print(\"Full data size:\", df.shape[0])\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Bag of Words processing to the split data\n",
    "We fit the vocabulary dictionary only for the training data. Here's a great explanation why we do this: https://sebastianraschka.com/faq/docs/scale-training-test.html. The basic idea is that we treat test data as new and unseen. Therefore, we cannot learn--by applying the fit() function--the vocabulary of this new data and must use the vocabulary learned from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer()\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem review\n",
    "\n",
    "The example in the tutorial is on finding the odds of a person having diabetes given that they got a positive result. Let the probability of having diabetes, denoted by $P(D)$, be 0.01. This is often referred to as the __base rate__ or the __prior__ probability. Assume that the test is %90 accurate. This means that the probability of testing positive given that you have diabetes is $P(Pos|D) = 0.9$. Alternatively, the probabily of testing negative given the person doesn't have diabetes is $P(Neg|~D) = 0.9$.\n",
    "\n",
    "The probability we are interested in is\n",
    "\n",
    "$$P(D|Pos) = \\frac{P(Pos|D)P(D)}{P(Pos)}$$\n",
    "\n",
    "$P(Pos)$ is the probability of testing positive regardless of whether you have diabetes or not. People without diabetes can still test positive because the test is imperfect. The formula for $P(Pos)$ is\n",
    "\n",
    "$$P(Pos) = P(D)P(Pos|D) + P({\\sim}D)P(Pos|{\\sim}D)$$\n",
    "\n",
    "Intuitively, $P(D)P(Pos|D)$ is the probability of testing positive _and_ having diabetes. $P(~D)P(Pos|~D)$ is the probability of still testing positive _and_ not having diabetes. Their sum, therefore, is just the probability of testing positive, whether or not you have diabetes.\n",
    "\n",
    "Putting all of this together the probability of having diabetes given that you tested positive is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333336\n"
     ]
    }
   ],
   "source": [
    "prob_d = 0.01\n",
    "prob_pos_d = 0.9\n",
    "prob_neg_no_d = 0.9\n",
    "prob_pos = prob_d * prob_pos_d + (1 - prob_d) * (1 - prob_neg_no_d)\n",
    "\n",
    "prob_d_pos = (prob_pos_d * prob_d) / prob_pos\n",
    "\n",
    "print(prob_d_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the test being accurate 90% of the time, your probability of actually having diabetes is quite low. This is because the base rate incidence of having diabetes, independent of whether you tested positive or not, is only 1%. \n",
    "\n",
    "In this example, there is only feature with which we can use to predict whether you have diabetes. It is conceivable that many other features can predict the disease. To extend our example, suppose we also had data on whether your parents also had diabetes. The posterior probability is now as follows\n",
    "\n",
    "$$P(D|Pos, FamHist) = \\frac{P(Pos, FamHist|D)P(D)}{P(Pos, FamHist)}$$\n",
    "\n",
    "where $FamHist$ is an indicator equal to 1 if either one of your mother or father had diabetes. $P(Pos, FamHist)$ is the joint probability of a given individual having some outcome of a a test result and their family history. Repeatedly applying conditional probabilities, we can rewrite the object in the numerator as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(Pos, FamHist|D)P(D) & = P(Pos, FamHist, D)  \\\\\n",
    " & = P(Pos|FamHist, D)P(FamHist, D)  \\\\\n",
    " & = P(Pos|FamHist, D)P(FamHist|D)P(D)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This can get unwieldy if we had 3, or 4, or in most cases, 10 or more features. As a simplification, assume that features are _conditionally_ independent given that you have diabetes. In other words, $P(Pos|FamHist, D) = P(Pos|D)$. The formula then simplifies to\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(Pos, FamHist|D)P(D) & = P(Pos|D)P(FamHist|D)P(D)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This is much easier to work with because with N features the formula is just\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(x_1,...,x_N|D)P(D) & = \\prod_{i=1}^{N}P(x_i|D)P(D)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "which is an formula to work with than if we didn't make the conditional independence assumption. \n",
    "\n",
    "This assumption is what makes this formulation of the Bayes Theorem naive. It is naive because it is conceivable that ones family history of diabetes, for whatever reason, affects the outcome of the test results. Perhaps this is not the best example for this, but imagine we are trying to predict whether house price will be high or low (based on some exogenously established threshold) given the number of rooms and the size of the house in square feet. The size of your house certainly affects how many rooms your house will have, thus, it unreasonable to assume that number of rooms is conditionally independent of house size. This is the price to pay for tractability.\n",
    "\n",
    "Going back to the diabetes example, the final formula of the posterior probability given conditionally independent features is\n",
    "\n",
    "$$P(D|Pos, FamHist) = \\frac{P(Pos|D)P(FamHist|D)P(D)}{P(Pos, FamHist)}$$\n",
    "\n",
    "The denominator is just $P(Pos, FamHist) = P(D)P(Pos,FamHist|D) + P({\\sim}D)P(Pos,FamHist|{\\sim}D)$\n",
    "\n",
    "In summary, Naive Bayes is just an extension of Bayes Theorem where the features are conditionally indepedent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
