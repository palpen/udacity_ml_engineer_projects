{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Boosting Techniques\n",
    "## AdaBoost, Gradient Boosting Classifier, and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've read a lot of articles about XGBoost and how it just blows away all other boosting algorithms (and non-boosting algorithms). My goal in this notebook is to see this for myself. This notebook compares the performance of three boosting algorithms using the [CharityML](https://archive.ics.uci.edu/ml/datasets/Census+Income) dataset. These three algorithms are AdaBoost, Scikit-Learn's implementation of gradient boosting, and its improved version, XGBoost.\n",
    "\n",
    "The boosting technique is an approach used in ensemble learning which trains a series of learning models---called weak learners---that improve with each iteration. The weak learners are trained in sequence with each weak learner accounting for the mistakes of the previous weak learner and as a result improving its own predictive performance. The final predictions are then made based on the aggregate predictions of each individual weak learner.\n",
    "\n",
    "The key difference between AdaBoost and gradient boosting is in the way each weak learner improve on its predecessor. For AdaBoost, the points that are misclassified by the current weak learner are assigned greater weight in the next iteration. This changes the distribution of points resulting in greater emphasis on the avoidance of misclassifying these points again when training the next weak learner. \n",
    "\n",
    "Gradient boosting takes a slightly different approach. Instead of changing the weights of the misclassified points of the previous weak learner, the current weak learner adjusts the prediction itself. This adjustment are based on the error residuals calculated from estimates made by the predecessor weak learner. It turns out that this method of learning is very similar to the way the gradient descent algorithm work, hence the name. This is more clearly explained in the [informal introduction](https://en.wikipedia.org/wiki/Gradient_boosting) on gradient boosting on Wikipedia. The advantage of gradient boosting over AdaBoost is it lets you use any differentiable loss function. With AdaBoost, you're limited to using just exponential loss.\n",
    "\n",
    "XGBoost is everything that the vanilla gradient boosting algorithm wanted to be growing up. It's the friend that got into all the good schools, received offers at all the cool jobs, and dated all the good-looking and smart people. In the domain of machine learning competition, it is the algorithm behind many winning solutions in competitions using structured data.\n",
    "\n",
    "XGBoost is just a gradient boosting algorithm with improved features. This [article](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) clearly outlines its advantages. One particularly important feature of XGBoost is its capacity to scale. Boosting algorithms, by nature, are sequentially trained. This makes it difficult to parallelize the algorithm. \n",
    "\n",
    "\n",
    "### References\n",
    "1. [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "2. [Good video explaining gradient boosting](https://www.youtube.com/watch?v=sRktKszFmSk)\n",
    "3. [Tutorial on XGBoost][2]\n",
    "\n",
    "### Installing XGBoost\n",
    "\n",
    "The original instructions to install XGBoost can be found [here][1]. I had trouble installing it following these instructions which I suspect is because I'm using the Anaconda distribution. I was able to install it using this command\n",
    "\n",
    "```\n",
    "conda install -c conda-forge xgboost=0.6a2\n",
    "``` \n",
    "\n",
    "which I found on Stack Overflow [here][3]. \n",
    "\n",
    "[1]: http://xgboost.readthedocs.io/en/latest/build.html\n",
    "[2]: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "[3]: https://stackoverflow.com/questions/43464454/install-xgboost-on-anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:04:09) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_level</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass education_level  education-num       marital-status  \\\n",
       "0   39          State-gov       Bachelors           13.0        Never-married   \n",
       "1   50   Self-emp-not-inc       Bachelors           13.0   Married-civ-spouse   \n",
       "\n",
       "         occupation    relationship    race    sex  capital-gain  \\\n",
       "0      Adm-clerical   Not-in-family   White   Male        2174.0   \n",
       "1   Exec-managerial         Husband   White   Male           0.0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country income  \n",
       "0           0.0            40.0   United-States  <=50K  \n",
       "1           0.0            13.0   United-States  <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import supplementary visualization code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the Census dataset\n",
    "data = pd.read_csv(\"census.csv\")\n",
    "\n",
    "# Success - Display the first record\n",
    "display(data.head(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Split the data into features and target label\n",
    "income_raw = data['income']\n",
    "features_raw = data.drop('income', axis = 1)\n",
    "\n",
    "# Log-transform the skewed features\n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
    "\n",
    "\n",
    "# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "\n",
    "# Encode the 'income_raw' data to numerical values\n",
    "income = pd.get_dummies(income_raw)\n",
    "income = income['>50K']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 36177 samples.\n",
      "Testing set has 9045 samples.\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                    income, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data using slicing with \n",
    "    # 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time()\n",
    "    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time()\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # Get the predictions on the test set(X_test), then get \n",
    "    # predictions on the first 500 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:500])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # Compute accuracy on training (the first 500 training samples) and test set\n",
    "    results['acc_train'] = accuracy_score(y_train[:500], predictions_train)\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # Compute F-score on the training (first 500) and test set\n",
    "    results['f_train'] = fbeta_score(y_train[:500], predictions_train, beta=0.5)\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of trees\n",
    "* Sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance across sample size\n",
      "AdaBoostClassifier trained on 361 samples\n",
      "AdaBoostClassifier trained on 3617 samples\n",
      "AdaBoostClassifier trained on 36177 samples\n",
      "\n",
      "Performance based on number of trees on 100% sample\n",
      "AdaBoostClassifier trains with 10 number of trees\n",
      "AdaBoostClassifier trains with 30 number of trees\n",
      "AdaBoostClassifier trains with 50 number of trees\n",
      "AdaBoostClassifier trains with 70 number of trees\n",
      "AdaBoostClassifier trains with 100 number of trees\n",
      "AdaBoostClassifier trains with 150 number of trees\n",
      "\n",
      "Performance across sample size\n",
      "GradientBoostingClassifier trained on 361 samples\n",
      "GradientBoostingClassifier trained on 3617 samples\n",
      "GradientBoostingClassifier trained on 36177 samples\n",
      "\n",
      "Performance based on number of trees on 100% sample\n",
      "GradientBoostingClassifier trains with 10 number of trees\n",
      "GradientBoostingClassifier trains with 30 number of trees\n",
      "GradientBoostingClassifier trains with 50 number of trees\n",
      "GradientBoostingClassifier trains with 70 number of trees\n",
      "GradientBoostingClassifier trains with 100 number of trees\n",
      "GradientBoostingClassifier trains with 150 number of trees\n",
      "\n",
      "Performance across sample size\n",
      "XGBClassifier trained on 361 samples\n",
      "XGBClassifier trained on 3617 samples\n",
      "XGBClassifier trained on 36177 samples\n",
      "\n",
      "Performance based on number of trees on 100% sample\n",
      "XGBClassifier trains with 10 number of trees\n",
      "XGBClassifier trains with 30 number of trees\n",
      "XGBClassifier trains with 50 number of trees\n",
      "XGBClassifier trains with 70 number of trees\n",
      "XGBClassifier trains with 100 number of trees\n",
      "XGBClassifier trains with 150 number of trees\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf_A = AdaBoostClassifier()\n",
    "clf_B = GradientBoostingClassifier()\n",
    "clf_C = XGBClassifier()\n",
    "\n",
    "samples_100 = len(y_train)\n",
    "samples_10 = int(samples_100 * .10)\n",
    "samples_1 = int(samples_100 * .01)\n",
    "\n",
    "models = [clf_A, clf_B, clf_C]\n",
    "sample_sizes = [samples_1, samples_10, samples_100]\n",
    "num_trees = [10, 30, 50, 70, 100, 150]\n",
    "# num_trees = [10, 30]\n",
    "\n",
    "results_samples = {}\n",
    "results_num_trees = {}\n",
    "\n",
    "# performance across sample sizes\n",
    "for clf in models:\n",
    "\n",
    "    clf_name = clf.__class__.__name__\n",
    "    \n",
    "    print('Performance across sample size') \n",
    "    results_samples[clf_name] = {}\n",
    "    for i, sample in enumerate(sample_sizes):\n",
    "        \n",
    "        print(\"{} trained on {} samples\".format(clf_name, sample))\n",
    "        results_samples[clf_name]['size_{}'.format(sample)] = \\\n",
    "        train_predict(clf, sample, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"Performance based on number of trees on 100% sample\")\n",
    "    results_num_trees[clf_name] = {}\n",
    "    for i, num_tree in enumerate(num_trees):\n",
    "        \n",
    "        print(\"{} trains with {} number of trees\".format(clf_name, num_tree))\n",
    "        clf = clf.set_params(n_estimators=num_tree)\n",
    "        results_num_trees[clf_name]['num_tree_{}'.format(num_tree)] = \\\n",
    "        train_predict(clf, samples_100, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdaBoostClassifier': {'acc_test': 0.85760088446655613,\n",
      "                        'acc_train': 0.84999999999999998,\n",
      "                        'f_test': 0.72455089820359275,\n",
      "                        'f_train': 0.71153846153846156,\n",
      "                        'pred_time': 0.06020188331604004,\n",
      "                        'train_time': 1.4236290454864502},\n",
      " 'GradientBoostingClassifier': {'acc_test': 0.86301824212271971,\n",
      "                                'acc_train': 0.85666666666666669,\n",
      "                                'f_test': 0.7395338561802719,\n",
      "                                'f_train': 0.73412698412698407,\n",
      "                                'pred_time': 0.020383119583129883,\n",
      "                                'train_time': 7.743292331695557},\n",
      " 'XGBClassifier': {'acc_test': 0.86323935876174684,\n",
      "                   'acc_train': 0.85333333333333339,\n",
      "                   'f_test': 0.74354306519513036,\n",
      "                   'f_train': 0.72916666666666674,\n",
      "                   'pred_time': 0.03812718391418457,\n",
      "                   'train_time': 2.261685848236084}}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "pp.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Create figures to summarize findings\n",
    "* Describe findings\n",
    "* Add more references that you found useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
